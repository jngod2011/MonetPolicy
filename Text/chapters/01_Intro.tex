% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % 			  Intro  			% % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %	

Monetary policy actions have very influential effects on capital markets. Especially target rate changes impact the bond market directly and hence affect interest rates of all maturities. \textcite{Ellingsen.2001}, however, call attention to the limitations of explanatory models and dissent among scholars about the ramifications of target rate adjustments by central banks, in particular when it comes to the response of the yield curve. More precisely, empirical evidence shows short and long term market rates moving in the same as well as opposite directions after a monetary policy change which to this point, no coherent theory can explain.

In order to eradicate this shortcoming, they introduced a model based on the presumption that a change in monetary policy can be traced back to two intentions. Either, new information about the economy has been discovered and thus monetary policy authorities respond to it or, their objective functions, i.e. their preferences, changed. The former case is referred to as \textit{endogenous} response to new information while the latter is characterised as \textit{exogenous} shift in preferences. While short rates co-move closely with the target rate in both cases, the model of \textcite{Ellingsen.2001} predicts the long end of the yield curve to  move in the same direction whenever monetary authorities endogenously respond to new information about the state of the economy while long rates move in the opposite direction of the target rate, i.e. a tilt in the yield curve when a policy action can be traced back to an exogenous shift in preferences of the respective monetary authority. The yield curve response happens through actions of bond market participant who update their expectations about future interest rate targets upon observing and interpreting the policy action and thus, price the information into the yield curve. 

% Economic motivation
Implementing this distinction into a model resolves a mismatch between macroeconomic intuition and empirical observations. The former would imply that short and long rates are linked together through arbitrage considerations as well as a change in inflation expectations by market participants. Hence, long interest rates should fall when the target rate is increased as \textcite{Ellingsen.2003} report, mainly due to a change in inflation expectations. Empirical research shows, however, that the yield curve sometimes tilts while in most cases long and short maturity rates move in the same direction. As asset prices change once a target rate adjustment is announced, their model succeeds in disentangling how bond market participants change their views about the state of the economy and the central bank's objectives as a response to the monetary authority's actions. Further evidence for this theory is \textcite{Gurkaynak.2004} who find that monetary policy actions as well as their accompanying statements influence asset prices.

On top of setting up a model to describe the influence of monetary policy on the term structure of interest rates, \textcite{Ellingsen.2001} perform empirical tests in order to support their theory. While some authors such as \textcite{Peersman.2002} and \textcite{Evans.1998} rum some variations of Vector Autoregression analysis to determine changes in policy preferences, \textcite{Ellingsen.2001} utilize the interpretation of bond traders and analysts as described in a column of the Wall Street Journal (WSJ) surrounding the day of a target rate adjustment by the Federal Reserve Bank as described in \textcite{Ellingsen.2003} and use this classification as input to a straightforward regression analysis.

The authors point out that their classification approach is limited to the extent that the daily frequency of the newspaper column might not reflect the immediate opinion of the bond traders whose move seconds after the decision would be the cleanest measure for the classification; the trader's opinion would furthermore not be biased through interpretations by others at that point. Apart from that, the source of input for the classification is very limited to one newspaper and a few journalists. Even though it can be argued that continuity and consistency support a well founded classification, it might be biased through limited sample size and the twofold human interaction while interviewing traders and interpreting the finished article. The latter implies that the journalist could have misunderstood the trader's initial interpretation of the policy change and, on top of that, the published article might not unambiguously convey the trader's interpretation.

Both of these shortcomings can be overcome by applying techniques developed to deal with big data, namely, Natural Language Processing (NLP) and Machine Learning (ML), assuming that opinions of traders and journalists converge to the prevalent one with number of information. Firstly, the limited sample that can be analysed by hand can be extended by training an algorithm. Secondly, setting exact rules ex-ante ensures a consistent interpretation of articles and thus, removes one human interaction that is exposed to misinterpretation. Furthermore, outliers in the articles have a smaller impact on the final classification due to the increased sample size and inclusion of different sources. Finally, even though the articles analysed still not necessarily reflect the bond trader's immediate opinion, it can be argued that the prevailing interpretation of a policy event by bond traders will win through in the mass of articles covering the event and thus, serve as a good approximation of the immediate interpretation. 

Even though methods built upon these have been applied in economics and finance, the full extent of possibilities has not yet reached these fields of academic research. Most applications only consider specific documents such as the employment report \parencite{Hautsch.2002,Hess.2004} while others merely look at the existence of such reports and effects around their release \parencite{Bomfim.2003,Hautsch.2011,Lucca.2015}; \textcite{Tetlock.2007}, like \textcite{Ellingsen.2001}, analyses daily content from only one WSJ column where sometimes even different topics are discussed and the wanted information is not present. On the other hand, \textcite{Manela.2017} look at newspaper articles over time but only consider those from the front page in order to make the sample size feasible for analysis. This way, the selection can be interpreted to cover the most important news as decided by the publishing agency but since they have completely different goals as a researcher looking at impact of news, this selection procedure is arbitrary and important information might be lost. Conversely, analysing all articles is computationally not feasible and furthermore will include too much noise through statements irrelevant for the respective topic at hand.

This thesis aims to replicate the results of the theoretical model of \textcite{Ellingsen.2001} using NLP and ML techniques. Since the availability of newspaper articles was very limited during their sample period between 1988 and 2003, I analysed newspaper articles between 2001 and 2017 for which data quality was high throughout the sample. Alas, the latter period is heavily affected by the recession following the financial crisis as well as quantitative easing (QE) efforts undertaken by monetary authorities. Hence, rather than finding evidence for the economic model about the connection of monetary policy and market interest rates, this thesis evaluates the model's validity during unprecedented economic conditions and central bank interventions. On top of that, itv establishes state of the art methods to deal with today's increased amount of data and introduces a strategy to identify and classify relevant newspaper articles to perform text analytics on a sample of feasible size. 

The remainder of the paper is organized as follows. Section~\ref{sec:Lit} provides an overview over monetary policy and its goal during the past decades as well as introducing methodology based on text analysis. The utilized data is discussed in section~\ref{sec:Data} while section~\ref{sec:Meth} goes into detail about the empirical strategy applied to answer the research question; section~\ref{sec:Res} presents the results of the empirical analysis. A critical evaluation of the chosen procedure can be found in section~\ref{sec:Disc}; section~\ref{sec:Conc} concludes.
%
% add http://www.sciencedirect.com/science/article/pii/S0304393201000551 maybe?
%