% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % 			  Intro  			% % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %	

Monetary policy actions are among the most influential recurring occurrences when it comes to movements on capital markets. In particular, target rate changes impact the bond market directly and hence affect interest rates of all maturities. \textcite{Ellingsen.2001}, however, call attention to the limitations of explanatory models and dissent among scholars about the ramifications of target rate adjustments by central banks, in particular when it comes to the response of the yield curve. 

In order to eradicate this shortcoming, they introduced a model based on the presumption that a change in monetary policy can be traced back to two intentions. Either, new information about the economy has been discovered and thus monetary policy authorities respond to it or, their objective functions, i.e. their preferences, changed. The former case is referred to as \textit{endogenous} while the latter is characterised as \textit{exogenous}.

On top of that, \textcite{Ellingsen.2001} perform empiric tests in order to support their theory. While some authors such as \textcite{Peersman.2002} and \textcite{Evans.1998} perform some variations of VAR analysis to determine changes in policy preferences, \textcite{Ellingsen.2001} utilize the interpretation of bond traders and analysts as described in a column of the Wall Street Journal (WSJ) surrounding the day of a target rate adjustment by the Federal Reserve Bank as described in \textcite{Ellingsen.2003} and use this classification as input to a wider regression analysis.

The authors point out that their classification approach is limited to the extend that the daily frequency of the newspaper column might not reflect the immediate opinion of the bond traders who's move seconds after the decision would be the cleanest measure for the classification; the trader's opinion would furthermore not be biased through interpretations by others at that point. Apart from that, the source of input for the classification is very limited to one newspaper and a few journalists. Even though it can be argued that continuity and consistency support a well founded classification, it might be biased through limited sample size and the twofold human interaction while interviewing traders and interpreting the finished article. 

Both of these shortcomings can easily be overcome by applying techniques developed to deal with big data, namely, Natural Language Processing (NLP) and Machine Learning (ML), assuming that opinions of traders and journalists converge to the prevalent one with number of information. Even though methods built upon these have been applied in economics and finance, the full extend of possibilities has not yet reached these fields of academic research. Most applications only consider specific documents such as the employment report \parencite{Hautsch.2002,Hess.2004} while others merely look at the existence of such reports and effects around their release \parencite{Bomfim.2003,Hautsch.2011,Lucca.2015}; \textcite{Tetlock.2007}, like \textcite{Ellingsen.2001}, analyses daily content from only one WSJ column where sometimes even different topics are discussed and the wanted information is not present. On the other hand, \textcite{Manela.2017} looks at newspaper articles over time but only considers those from the front page in order to make the sample size feasible for analysis. This way, the selection can be interpreted to cover the most important news as decided by the publishing agency but since they have completely different goals as a researcher looking at impact of news, this selection procedure is arbitrary and important information might be lost. Conversely, analysing all articles is computationally simply not feasible and furthermore will include too much noise through statements irrelevant for the respective topic at hand.

By trying to replicate the results of \textcite{Ellingsen.2001} using NLP and ML techniques, this paper thus aims to find evidence for their theoretical model in the period of unprecedented economy conditions, establishes state of the art methods to deal with today's increased amount of data and introduces a strategy to identify relevant newspaper articles to perform text analytics on a sample of feasible size. 

The remainder of the paper is organized as follows. Section~\ref{sec:Lit} provides an overview over monetary policy and its goal as well as introducing methodology based on text analysis while section~\ref{sec:Meth} goes into detail about the empirical strategy applied to answer the research question. The utilized data is discussed in section~\ref{sec:Data} and section~\ref{sec:Res} presents the results of the empirical analysis. A critical evaluation of the chosen procedure can be found in section~\ref{sec:Disc}; section~\ref{sec:Conc} concludes.
%
