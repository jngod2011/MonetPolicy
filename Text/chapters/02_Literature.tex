% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % 			Literature 			% % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %	

Rather than suggesting alternative econometric methods to find empirical evidence to the model of market interest rates and monetary policy introduced in \textcite{Ellingsen.2001}, this thesis attempts to evaluate their classification strategy as stated in \textcite{Ellingsen.2003} by removing human influence and automating the task to the largest extent possible to the best of the author's knowledge. For that reason, the methodology suggested in section~\ref{sec:Meth} bases on techniques used in the area of text analytics and applies it to newspaper articles about monetary policy. The basic concepts of both areas are laid out in the following.

\subsection{Monetary policy}

The primary objectives of monetary policy as well as actions in order to achieve their goals have varied over time, development status and  political system of an economy. \textcite{Mishkin.2007} makes a strong case about how the past decades after the Great Depression and their experiences have contributed to the current monetary system in the developed world, in particular in the U.S.A. as described below. 
% start with this since it's the aim of the paper, background for method later

\subsubsection{The beginnings of inflation targeting}
After the events of the Great Depression, the prevalent approach among monetary authorities was Keynesian and later influenced by \textcite{Samuelson.1960}. In their interpretation of the Phillip's curve, a trade-off between unemployment and inflation has to be resolved in the long-run. Hence, monetary and fiscal policy had the objective to achieve full employment at the cost of a slight rise in inflation. Alas, inflation exceeded the ten percent mark and employment even decreased compared to its level in the 1950s. 

Milton Friedman and his stream of monetarists argued, however, that there was no long-run trade-off but rather a natural rate of unemployment would be the equilibrium, irrespective on inflation. For this reason, monetary policy should target inflation instead of output, the determinant for employment, by ensuring a steady growth in the money supply. This argument was later confirmed by Robert Lucas' rational expectations theory. With the oil price shock of 1973, awareness of the importance of a nominal anchor rose as the high costs that inflation accommodates became more apparent. The mechanism of such an anchor would facilitate low and stable inflation expectations that lead to stable price and wage setting behaviour of firms, decreasing level and volatility of inflation.

With the realisation that expansive monetary policy does not lead to higher output in the long run, inflation is costly and a nominal anchor is beneficial, many industrialised nations adapted monetary targeting in the mid-1970s. Keeping inflation under control using this strategy hinges upon one critical assumption; there has to be a strong relationship between the goal variable, i.e. inflation or nominal income, and the target aggregate. During the 1980s, it became apparent that this was not valid any more and should be abandoned. %could also have been not properly persued before
Observing how successful German and Swiss central banking had become through the adoption of very transparent policy moves using target ranges, \textcite{Mishkin.2007} writes, a numerical and clearly communicated long-run goal would support in creating less volatile inflation expectations and still left the central bank enough leeway to deal with short-run fluctuations. 

Nevertheless, the prevalent monetary targeting strategy faced difficulties due to its weak relationship between money supply and nominal income, making it impossible to reach the desired inflation outcome. Furthermore, monetary aggregates were no longer a useful signal about the attitude of monetary authorities. As a result, monetary targeting could not properly be used as a nominal anchor and support steering inflation expectations in the vast majority of cases. 

In order to make use of advantages of monetary targeting compared with the German and Swiss communication strategy as well as providing a strong nominal anchor, inflation became the new target during the 1990s in many developed economies. Research by \textcite{Kydland.1977,Calvo.1978,Barro.1983} showed that a strong nominal anchor such as inflation could even solve the time-inconsistency problem. The defined long-run commitment to price stability, expressed through a numerical value, holds central banks accountable and makes their performance easy to assess and thus less vulnerable to influence for politicians who are  incentivised to use central bank tools for short-run expansive policy. Output, and thus employment, are still apparent in the monetary authorities' objective function as \textcite{Svensson.1997} showed. It does, however, consider the former's long-run perspective rather than cyclical behaviour as in the 1960s and 1970s and is thus referred to as \textit{flexible inflation targeting} by \textcite{Mishkin.2007}.

% references here?
Naturally, it is debatable whether central banks should adapt their strategy subject to current developments. The real estate bubble in the U.S.A. stirred question about how to react to asset prices, including exchange rates, while the following period of interest rates at the zero-lower-bound challenged central banks worldwide as their traditional instruments such as target rates could no longer be applied. Thus, central banks such as the Fed and the ECB starting purchasing government bonds in order to raise inflation while smaller economies such as Sweden and Switzerland introduced slightly negative interest rates and the Czech Central Bank pegged the Czech Crown to the Euro. Furthermore, as \textcite{Mishkin.2007} points out, the prevalent opinion diverges about what extend of central bank transparency is still beneficial to an economy. He argues that the relative weights of inflation and output in the goal function should stay occult as the public would not be able to identify when a central bank reacts to economic events and when it changes weights. As shown by \textcite{Ellingsen.2001}, reactions on the bond market indicated, however, that markets make inferences from Fed statements which rationale is underlying a target rate change. % blabla methodology to check this in the following

% interest rate movements usually well predicted, more diffcult when (find the statement in paper) and size (?)
% \textcite{Cecchetti.2006} \dots

\subsubsection{Recent history of Fed actions and target rates}

Once the Fed started targeting the interest rate in 1988, transparency of policy moves increased compared to the period of money stock measures as pointed out in \textcite{Ellingsen.2003}. The aftermath of the financial crises erupting after the collapse of the real estate bubble in the U.S.A. in 2007 had the most influential central banks cut interest rates to their natural lower bound, hindering the traditional monetary transition mechanism, and introduced alternative measures to conduct monetary policy. Table~\vref{tab:FEDfundstgt} lists the Federal Open Market Committee's (FOMC) target federal funds rate or range, change (basis points) and level as published on \textcite{Fed.OMOs}. While the target rate reached their peak in June 2006, two and a half years later, after the Lehman Brothers collapse, interest rates reached their all-time low and a new era of Zero Interest Rate Policy (ZIRP) commenced.

As \textcite{Fawley.2013} state, the Fed executed measures to stimulate economic growth at a time of short rates approaching zero accompanied by the ECB, BOJ and BOE. Since the importance of banks and the bond market varied across their respective economies, the Fed and BOJ focussed their efforts on bond purchases while the other two lent to banks directly. Table~\vref{tab:FedOMOs} summarises the Fed's engagements to counter the repercussions of the financial crisis while their target rate remained at the Zero Lower Bound (ZLB). 
%
% Fed OMOs from January 2002 until June 2017?
% OR figure~\dots depicts a time line of Fed target rate actions and QE as in Fawley
% https://tex.stackexchange.com/questions/196794/how-can-you-create-a-vertical-timeline
%
\begin{sidewaystable}[htbp]
	\small
	\caption[Important Announcements by the Federal Reserve]{Federal Reserve open market operations during and after the financial crisis based on \textcite[p.~61]{Fawley.2013}.}
	\label{tab:FedOMOs}
	\centering
	\begin{tabular}{ll p{8cm}l}
		\toprule
		Date & Program & Description & Target rate  \\
		\midrule
		25.11.2008 & QE1 (12/2008-03/2010) & Fed announces to purchase \$100bn in government-sponsored enterprise (GSE) debt and \$500bn
			in mortgage-backed securities (MBS). & 1.00\% \\
		18.03.2009 & QE1 (12/2008-03/2010) & Fed announces to purchase \$300bn in long-term Treasuries and additional \$750bn in MBS and \$100bn in GSE. & 0.00-0.25\% \\
		03.11.2010 & QE2 (11/2010-06/2011) & Fed announces to purchase \$600bn in Treasuries &  0.00-0.25\% \\
		21.09.2011 & Operation Twist (09/2011-12/2012) & Fed announces to purchase \$400bn of Treasuries with remaining maturities of 6 to 30 years and divest \$400bn with maturities at most 3 years. & 0.00-0.25\% \\
		20.06.2012 & Operation Twist (09/2011-12/2012) & Program to purchase and divest \~\$45bn/month extended to the end of 2012. & 0.00-0.25\% \\
 		13.09.2012 & QE3 (09/2012-10/2014)& Fed announces to purchase \$40bn of MBS per month until the outlook for the labour market improves in the context of price stability. & 0.00-0.25\% \\
 		16.12.2015 & End of ZIRP (12/2008-12-2015) & Economic activity has been expanding at a moderate pace, leading to the Fed opting for an increase in the Federal Funds Rate. & 0.25-0.5\% \\
		\bottomrule
	\end{tabular}
\end{sidewaystable}
%

% DESCRIBE QE programs etc. QE1(p10), QE2(p22)
Over the course of time, the Fed executed four major large-scale asset purchase (LSAP) programs, usually referred to as QE1, QE2, Operation Twist (Maturity Extension Program and
Reinvestment Policy) and QE3. As \textcite{Fawley.2013} point out, the Fed kept its balance sheet size by reinvesting maturing assets into treasuries at first, and later MBS and GSE debt into MBS. All four programs combined led to a threefold increase of the monetary base compared to pre-crisis levels while, due to the comprehensive augmentation of excess reserves through banks, broader aggregates increased to a more reasonable extent. 

As stated in \textcite{Blinder.2010}, the first Fed intervention, QE1, was meant to change the composition of the Fed's portfolio in order to increase liquidity at the capital markets, in particular housing credit markets, by divesting of Treasuries and purchase of less liquid assets such as GSE debt and MBS. % QE2 no surprise \textcite[p.73]{Fawley.2013}
By contrast, QE2 aimed at raising inflation and long-term real interest rates and was conducted through the liabilities side of the Fed's balance sheet as stated in \textcite{Blinder.2010} who emphasizes the Treasury's borrowing activities, indicating a combination of monetary and fiscal policy to achieve the goal of price stability. Further increasing it's balance sheet through lending operations, total Fed reserves more than doubled from \$907bn in September 2008 to \$2.214bn in November 2008 \parencite[p.~468]{Blinder.2010}. \textcite{Blinder.2010} also interprets these quantitative easing efforts as effective, at least in parts, as short-term as well as long-term interest rate spreads smoothed gradually.
While the Maturity Extension Program and Reinvestment Policy did not expand the monetary base but rather twisted the yield curve, QE3 was introduced while the Operation Twist was still running in order to improve labour market conditions.
% maybe add \textcite{Cecchetti.2006} and articles "Logic_of_MonetPol" and "FED_crashCourse" in Paper/Articles folder

%
\subsection{Text mining}

Empirical methods across disciplines have utilized quantitative, structured data for decades in order to find evidence for theoretical models, assess the effect of a change in policy and many others. While econometricians have applied knowledge about statistical distributions to empirical observations in order to make inferences about how trustworthy a result might be, the structure of the underlying data was always of major importance which is why many standardised software packages work in terms of two- and three dimensional arrays. 

With the raise of computer science, capabilities to analyse as well as availability of data skyrocketed and with it one of the oldest preservable communication means of mankind, written text. Even though grammatically correct sentences come natural to human beings, in order for them to make sense to a machine, a structure needs to be introduced which is the basis of text analytics. The standard way to do so for the classical applications, according to \textcite{Meyer.2008}, is based on term frequencies and distance measures. Even though there are standardized procedures when it comes to text analysis, a large variety of features require special attention to obtain meaningful results. For instance, whether the goal text is taken from a newspaper agency, blog, chat history or product feedback makes a huge difference. Different languages might have been used, terms might be misspelled, acronyms used, spam included and many others. \textcite{Meyer.2008} point out, however, that most software packages include the following five features and are thus applicable as basic framework. 

Preprocessing encompasses importing the data and cleaning it in such a way that it can be structured without noise through different use in grammar and language, generally. Association analysis refers to counting co-occurrence of terms while clustering assigns similar documents to groups. A general summary returns the major concepts within a text while categorisation classifies texts into predefined categories. 

In most cases of applications, after the data has been imported and cleaned, the structuring takes place in the form of a term-document matrix\footnote{In some cases it might make more sense to use a document-term matrix which is the inverse of the tdm.} (tdm). The latter can easily be created from a corpus, a collection of text documents, and takes the form of a matrix where the rows indicate documents and columns terms; the matrix elements are consequently the frequency of appearance for each term in every document. Once the texts are in this format, it is possible to write functions to deterministically identify documents and train models.

While clustering documents into different buckets according to specified or unspecified criteria, usually referred to as topic modelling, is one major text mining research area, this discussion does not contribute to the understanding of the methodology applied in this discussion and is thus excluded from the paper. A general overview and code examples for the software package \textit{R} can be found in \textcite[ch.~6]{Silge.2017}.

\subsubsection{Machine learning} % as used here

As indicated in chapter~\ref{sec:Intro}, ML techniques for data mining in economics do not receive the same attention as in other fields when it comes to testing theories. One major reason for this is probably a shortcoming in establishing causality as economic models do. \textcite{Athey.2017} present a recent overview paper on applied econometrics that cover precisely this issue and point out why the underlying data structure can solve this problem. Since the mere magnitude of data opens up possibilities to keep huge training and test sets, we are able to identify causal elements in contrast to classical regression models where a high level of identification is necessary to achieve significant and meaningful results. Thus, some ML techniques may offer an attractive way to serve as additional robustness check for the validity of economic models as well as empirical results from conventional econometrics. 

Like most textbooks, \textcite{Friedman.2001} split data mining techniques into supervised and unsupervised learning. The former is related to classical statistical literature in the sense that we have predictor/independent variables that influence certain response/dependent variables. Solely the way how this influence works differs among various methods. Formally, if random variables $(X,Y)$ are represented by some joint probability density $Pr(X,Y)$, supervised learning tries to determine the properties of the conditional density $Pr(Y|X) $. More specifically, finding 
%
\begin{equation*}
	\mu(x) = \underset{\theta}{\text{argmin }} E_{Y|X} L(Y,\theta)
\end{equation*}
%
that minimize the expected error at $x$ where $L(y,\hat{y})$ is some loss function. In unsupervised learning, on the other hand, one has a number of observations from a probability vector $X$ with joint density $Pr(X)$ from which the properties of the probability density are to be inferred directly without the help of a supervisor \parencite[pp.~485-486]{Friedman.2001}. In order to make the results of this analysis comparable to those of \textcite{Ellingsen.2003} who use clear conditions on their classification theory, this discussion will only utilize supervised learning techniques as commonly used in the field of sentiment analysis according to \textcite{Wiebe.1999}. 

%
\subsubsection{Sentiment analysis}

% lots of things have been done wrt sentiment in terms of pos/neg - adjust for engod/exog
The computational study of how opinions, sentiments, subjectivity, evaluations, attitudes appraisal, affects, views, emotions, etc. are expressed in text is referred to as opinion mining or sentiment analysis \parencite{Liu.2012}. According to the review of \textcite{Feldman.2013}, sentiment analysis can be on document, sentence or aspect level or of a comparative nature. While it might make sense to analyse the sentiment of different aspects or sentences in a product review, the classification of newspaper articles with respect to endogeneity versus exogeneity is most useful on a document level, simply because most articles are structured such that objective facts are presented and in some cases accompanied by the author's or some interviewed person's subjective interpretation on the underlying causes of an event. 

On this level, \textcite{Feldman.2013} points out the feasibility of supervised learning approaches as it can be assumed that there is a finite set of classes to which each document belongs, for instance endogenous and exogenous. When provided with training data, commonly used classification algorithms encompass na\"{i}ve Bayes (NB), k-nearest neighbours (KNN), Support vector machines (SVM) and maximum entropy (ME). Given the classification algorithm, new documents can be assigned the respective sentiment. As shown in \textcite{Pang.2002}, documents merely represented by a collection of words without structure still yield good accuracy. 

In general, an opinion lexicon has to be created in order to extract the sentiment of a statement. The former entails a list of words and expressions that are used to express people's subjective feelings and opinions. Apart from manually creating this list or access available dictionaries such as WordNet\textsuperscript{\textregistered} by \textcite{Fellbaum.1998,Esuli.2006}, it is possible to rely on syntactic patterns in large corpora as has been suggested in \textcite{Ding.2008,Hatzivassiloglou.1997,Kanayama.2006,Turney.2002,Yu.2003}; an elegant algorithm to find WordNet\textsuperscript{\textregistered} synonyms and antonyms is suggested in \textcite{Kamps.2004}. Even though \textcite{Feldman.2013} claims that sentiment lexicons are crucial for most sentiment analysis algorithms, it makes sense to use them with care in cases where sentiment does not refer to terms with clear and universe meaning, such as indicators for positive and negative features of a product or service. While the latter can usually be unambiguously derived from reviews, the endogeneity/exogeneity task builds on terms in a specific setting that might be used in different contexts where its sentiments diverge which is why the manual approach should be preferred to standardised lexica. 

Other approaches to solve the sentiment classification task in a different context include preselecting articles by sorting out objective statements that do not contribute to the classification \parencite{Wiebe.1999} or using an ontology tree based on aspects in every document. \textcite{Wang.2010} perform such an aspect-based analysis and assume an underlying linear combination the problem as Bayesian regression to determine an overall classification. Apart from the direct rating through so called regular opinions as laid out above, \textcite{Ding.2009,Ganapathibhotla.2008,Jindal.2006}, among others, analyse comparative opinions which are particularly important in product reviews. Finally, unsupervised learning can be applied; for instance, \textcite{Turney.2002} introduces an unsupervised three-step learning algorithm to mark a review as positive or negative\footnote{Neutral sentiment is ignored in many cases as a clear distinction from the non-neutral sentiments is not possible.} based on the log-likelihood ratio test which \textcite{Yu.2003} extend further. Another common tool in this field is Latent Dirichlet Allocation \parencite{Blei.2003} which is particularly helpful in topic modelling and can serve as a basis for sentiment analysis as in \textcite{Guo.2009} who develop a latent semantic association model.
%
