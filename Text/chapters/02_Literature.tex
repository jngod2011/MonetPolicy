% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % 			Literature 			% % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %	

\subsection{Monetary Policy}

See \textcite{Mishkin.2007} and \textcite{Cecchetti.2006} \dots
% start with this since it's the aim of the paper, background for method later
% FED target history, analyses about yield curve movements

% interest rate movements usually well predicted, more diffcult when (find the statement in paper) and size (?)

% subsection: wrap up of fed actions during time period

\subsection{Computational Linguistics and Machine Learning}

A good overview over techniques and applications of sentiment analysis is found in \textcite{Feldman.2013} and \textcite{Liu.2012} \dots

% why is this becoming more important and why can it be used for our purposes -> the Hess stuff
% lots of things have been done wrt sentiment in terms of pos/neg - adjust for engod/exog
% explain corpus
% pre-processing: remove stopwords, stemming, fuzzy matching (not needed here), etc

% Susan Athey: large magnitude of data -> not necessary anymore to identify 100 %, since magnitude so large, able to identify if there in many pieces

\subsection{Opinion Mining}

\textcite{Turney.2002} introduces an unsupervised three-step learning algorithm to mark a review as positive or negative\footnote{Neutral sentiment is ignored in many cases as a clear distinction from the non-neutral sentiments is not possible.} where first, \dots

% unsupervised: two approaches, see \textcite{Feldman.2013} - \textcite{Yu.2003} does sth interesting building on \textcite{Turney.2002}

\textcite{Pang.2002}, on the other hand, applies and compares three supervised learning techniques to classify reviews, namely na\"{i}ve Bayes, Maximum Entropy and Support Vector Machines (SVM). Other common classification algorithms entail Logistic Regression and k-nearest neighbours (KNN) \parencite{Feldman.2013}.

% identification of relevant articlesd not done, either first page... or just one column.... arbitrary! might lose info
% sure, important stff appears on front mages and in the same collumn, latter had issue (sometimes not mentioned);
% converesely, usiung all a) not feasible quantity & b) classification might be misslead (1. monet. pol. words, 2. suprise etc. words)
% former biased by construction as uncertainty is less news than a confident prediction
% unigrams and bigrams, n-grams

Most techniques use supervised learning according to \textcite{Wiebe.1999} which classifies sentences into the classes subjective or objective before determining the sentiment as the former classes usually do not express a positive or negative opinion. The importance of this observations stems from the fact that due to the vast amount of accessible data, computations are highly involved and thus can be made significantly more efficient by filtering articles with respect to their propensity to contribute to sentiment identification (or whatever other goal one might have in mind). 

% Turney 2002, thumbs up or down uses log-likelyhood ratio test to determine sentiment
% also: strength of sentiment - in my case maybe certainty? see Wilson 2004
% explicit aspects analysed, impicit not yet

Multilevel latent categorization as described in \textcite{Guo.2009} uses Latent Dirichlet Allocation (LDA) 

% careful with valence shifters, presuppositional items and modal auciliary verbs -> combine to unigram
% sarcasm, on the other hand should play less of an important role since 'boring' facts are reported -> different in blogs
% Opinion spam as in Jindal and Liu 2007, 2008 not an issue either

% include sentiment ontology tree for exog/endog classification (cf p. 106)
\textcite{Wang.2010} assume the overall review is a linear combination of its aspect ratings. Although they model the problem as Bayesian regression, \dots % p.112

In order to extract the sentiment of a statement, an opinion lexicon has to be created which entails a list of words and expressions that are used to express people's subjective feelings and opinions. Apart from manually creating this list or access available dictionaries such as WordNet\textsuperscript{\textregistered} by \textcite{Fellbaum.1998,Esuli.2006}, it is possible to rely on syntactic patterns in large corpora as has been suggested in \textcite{Ding.2008,Hatzivassiloglou.1997,Kanayama.2006,Turney.2002,Yu.2003}. See \textcite{Feldman.2013}. An elegant algorithm to to find WordNet\textsuperscript{\textregistered} synonyms and antonyms is suggested in \textcite{Kamps.2004} %explanation in \textcite{Feldman.2013}, p.86; put lexicon together through manual work and crowdsourcing

Apart from the direct rating through so called regular opinions as laied out above, \textcite{Ding.2009,Ganapathibhotla.2008,Jindal.2006}, among others, analyse comparative opinions. Since the latter is of merely of subordinate use for the classification of monetary policy task, we refer the reader to \textcite{Liu.2012} for an overview of the topic. 

%
As pointed out in \textcite{Hu.2004}, two techniques are most widely spread when it comes to discovering terms in corpora. Symbolic approaches that are based on terms such as noun phrases and their syntactic description encompass the first, statistical approaches the second while the latter make use of the fact that words composing a term a in close proximity to each other and reoccur. 

% DATA: 
% pre-selection of articles done through, among others, \textcite{Elastic.2015}
% Elastic: https://www.elastic.co/guide/en/elasticsearch/guide/current/getting-started.html

% For future: check which words are typical for authors, see Jane Austin stuff in the textbook