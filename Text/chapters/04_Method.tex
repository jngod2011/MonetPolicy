% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % 			Methodology			% % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %	

\textcite{Liu.2012} summarizes the most influential papers in the area of sentiment analysis or opinion mining, a procedure to determine whether a snippet of text, be it in the form of a commentary on a product, post on social media or article in a newspaper, communicates a positive, negative or neutral message about the topic at stake using natural language processing (NLP). Application possibilities of these tools are ample, ranging from businesses improving their products through online reviews to automated fraud and insider trading detection through monitoring analysts messaging behaviour and are thus of major importance for businesses, individuals and policy makers alike. 
Building on \textcite{Liu.2012,Feldman.2013,Silge.2017,Friedman.2001}, %AND others
I derive a procedure to extract an opinion on published articles by major international newspapers and agencies in order to determine the prevalent public opinion on the nature of a target rate move by the FED.

\subsection{Terminology}

As texts are a typical example of unstructured data, it needs to be converted to structured data in order to perform meaningful analyses. Therefore, we follow \textcite{Liu.2010} in defining an opinion as the quintuple\footnote{Even though the literature on opinion mining takes subjectivity and emotion into account \parencite{Wiebe.2000,Wiebe.2004,Riloff.2006}, we refrain from doing so as the source of the data comes from professional and reviewed media sources exclusively and the aim of the analysis is not to determine how individuals feel about a certain product or situation but rather extract the summary of public opinion from a newspaper article statement.} 
\begin{equation}
	(e_j, a_{jk}, so_{ijkl}, h_i, t_l)
\end{equation}
where $e_j$ is the target entity which forms the opinion target together with $a_{jk}$, an aspect of the former; $so_{ijkl}$ refers to the sentiment value of the opinion source, $h_i$, on $a_{jk}$ of $e_j$ at time $t_l$. 
% e.g. entity is FED action, aspect is interest rate decision
% omit opinion holders
Since the first step of the analysis, the identification of relevant articles as described in section~\ref{subsec:ArticleID}, determines the opinion target while time and source are given exogenously, we focus on the discovering the sentiment of each text snippet. 


\subsection{Article identification}\label{subsec:ArticleID}

% how to get stuff out of the database: Long, Zhang and Zhu (2010) & Ma and Wan (2010)

\begin{itemize}
	\item before 07: factiva by hand (+/- one day as in \textcite{Ellingsen.2003}; filter for topic \textit{monetary policy}, geographic location \textit{North America}, language \textit{English} and the words \textit{Fed}, \textit{Federal Reserve} and \textit{Interest Rate})
	\item after 07: same as above for now
	\item to be implemented: after 07: use before 07 articles and some after 07 articles determined through TA and let elastic chose similar articles around a date; see \textcite{Elastic.2015} 
	% DATA: 
	% pre-selection of articles done through, among others
	% Elastic: https://www.elastic.co/guide/en/elasticsearch/guide/current/getting-started.html
	% this way, elastic algo identifies relevant stuff over time and specific for the period after 07
	% For future: check which words are typical for authors, see Jane Austin stuff in the textbook
	\item refrain from using headlines (agencies just facts, comments which are essential have satirical touch)
\end{itemize}
%


\subsection{Pre-processing}

After reading in the data by collecting all text snippets allocated to one FOMC meeting, a few manipulations have to be undertaken in order to prepare the text such that it is feasible for quantitative analysis. The rationale for this is simply that a large part of the text does not contain useful information and would hence distort the results by adding too much noise and taking up computation power. This procedure is commonly referred to as \textit{pre-processing} and entails the removal of stop words and stemming as well as manipulations depending on the respective task.

Apart from a few technical manipulations to make the input into \textit{R} easier, the pre-processing in this study begins by discarding all articles in a language other than English. The removal of stop words, i.e. terms that inherit no intrinsic meaning, is performed in two consecutive steps\footnote{Another approach is based on term-weighting as described in \textcite{Silge.2017} accompanied by their published \textit{R} package \textcite{tidytextpackage}. Since our articles are pre-selected in an earlier step, this procedure is deemed unnecessary, however.}. % tf-itf: https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html
First, an algorithm is applied to tokenize every word in each text. These Part-of-Speech (POS) tags as presented in table~\vref{tab:POS_tags} are applied to the articles through a pre-trained model by \textcite{OpenNLP.2016} that assigns POS tags based on the probability of what the correct POS tag is for newspaper language and selects the one with highest probability. Once every word is tagged, those identified to have only subordinate or auxiliary purpose are discarded. Secondly, all remaining elements of the text are scanned for punctuation, numbers, unnecessary white space and de-capitalised while an extended built-in function filled with common stop words subtracts the remaining ones. Special care has been taken with valence shifters, presuppositional items and modal auxiliary verbs that have been combined to unigrams, i.e. terms without spaces. The pre-processing is concluded by stemming the terms left such that words from the same family in different conjugations are detected as equivalent\footnote{When it comes to customer reviews, sarcasm and opinion spam are two more important aspects to look for; since this exercise mainly contains facts about monetary policy and interpretation by market participants, I deem this problem to be of subordinate nature. In particular, since the vast magnitude of words per article relativises this issue.}. Once this manipulation is fulfilled, a corpus is formed from all pre-processed text files allocated to a target rate change. 
% As pointed out in \textcite{Hu.2004}, two techniques are most widely spread when it comes to discovering terms in corpora. Symbolic approaches that are based on terms such as noun phrases and their syntactic description encompass the first, statistical approaches the second while the latter make use of the fact that words composing a term are in close proximity to each other and reoccur. 
% Sentence-Document model see \textcite{McDonald.2007} (Sentiment_Proceedings2007_p470) 2.1 ff for notation


%
\subsection{Sentiment determination}

In order to identify the sentiment of each text piece as endogenous or exogenous, I apply a deterministic, count-based (CB) approach as well as state-of-the-art ML algorithms. While the latter have shown stable performance in text mining studies in different contexts, this exercise demands high precision and thus I chose not to rely on learning algorithms alone but created a deterministic algorithm that classifies target rate changes based on predefined terms as well as significance tests. 

%
\subsubsection{Count-based evaluation}

One classical way to analyse texts through predefined methods is assuming that terms with higher occurrence frequencies are more important than others. Because of the simplicity of this approach, it is widely used throughout the field as pointed out in \textcite{Meyer2008}. In order to design a promising approach, a list of words and expressions has to be set up which can be done in three different ways as discussed in section~\ref{sec:Lit}. The most straight forward one, which has been chosen here for its comprehensible and deterministic nature, being manually setting it up in a one-time effort. The sentiment defining words are chosen such that they are representative for a endogenous or exogenous event as defined in \textcite{Ellingsen.2003} together with a synonym finder. The polarity is reversed whenever a term is preceded by a negation. In contrast to \textcite{Ellingsen.2003}, endogenous events are not defined residually.
% use dimension correlation as in \textcite{Godbole.2007} for the descriptive stats
In this discussion, it is sufficient to look for simple quantitative occurrence of expressions since, intuitively, crucial events as well as comments accompanying FOMC statements appear across many articles as common in count-based evaluation. Furthermore, the binary nature of the classification problem leads to analysing increments as common in ratings, for instance, being negligible whereas dictionary-based methods usually find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text \parencite{Silge.2017}. 

As input, my function takes a pre-processed corpus consisting of articles surrounding one FOMC meeting in which a target rate change has been decided and announced as well as a list of terms that are indicative of endogenous or exogenous sentiment, respectively; finally, a confidence level has to be decided ex ante. Naturally, across dates, the input parameters apart from the corpi are equivalent in order to make results comparable and consistent. The function then compares occurrences of the respective terms with predefined sentiment to each document in a corpus and prints the amount of words with endogenous and exogenous sentiment, respectively, as well as their difference. Thus, the output of the function is a list of documents from every FOMC date which states by how much the number of endogenous words exceeds or deceeds that of exogenous words. 

Even though \textcite{Meyer2008} describe this procedure as sufficient, varying article length by construction has a huge impact on the absolute number of sentiment word appearances across documents surrounding one FOMC meeting. For that reason, I included a one-sided \textit{t}-test for comparison of means among the different sentiment words across documents per FOMC meeting. Given that both vectors are created from the same articles ranging from a few hundred to a few thousand words, each, and consist of positive integer values, applying a standard student \textit{t}-test seems to be justified. Depending on whether the test yields a significant difference in means, the function returns a final classification stemming from the deterministic, count-based function or flags it as \textit{ambiguous}; results are denoted in table~\vref{tab:Tab_Class}.

% interlude
Since across the data mining literature as pointed out in section~\ref{sec:Lit}, machine learning techniques receive more attention and trust than simple count-based approaches, I selected a few articles with obvious sentiment as training set which I extend with the text snippets from \textcite{Ellingsen.2003} and train the most common supervised learning techniques according to \textcite{Liu.2010} and \textcite{Feldman.2013}. Generally, train and test sets have to contain records that are representative of the entire dataset in order to yield internally valid parameters estimates which is why I took all articles for the training set from the remaining ones and selected a few across all dates. 

\subsubsection{Na\"{i}ve Bayes} % Elements of statistical learning: p. 211

\textcite[p.~211]{Friedman.2001} list the Na\"{i}ve Bayes (NB) classifier among the linear methods for classification, as a variant of linear discriminant analysis. As such, it adapts the loss function to the fact that the output variable $G$ is categorical such that prediction errors are penalised appropriately. Utilising a zero-one loss function in which every misclassification is charged one single unit, \textcite[pp.~20-21]{Friedman.2001} simplify the expected prediction error $EPE=E[L(G,\hat{G}(X))]$ such that the 
%
\begin{equation}
	\hat{G} = \underset{g\in \mathcal{G}}{max}\, Pr(g|X=x)
\end{equation}
%
conveys the intuition that classification is done to the most probable case in which the conditional discrete distribution $Pr(G,X)$ is used. NB enhances this approach by assuming that the inputs are conditionally independent in each class, i.e. that every class density is a product of marginal densities. Even though this assumption is generally not fulfilled as certain word combinations appear consistently, \textcite{Friedman.2001} as well as \textcite{Rish.2001} emphasise that it outperforms more sophisticated alternatives in many cases. One reason for this is that the prediction depends only on the maximum probability, not its actual value. Furthermore, dependencies cancel out in many cases when working with a large set of features. 

%
\subsubsection{Maximum entropy} % Elements of statistical learning: p.119
Maximum entropy (ME), on the other hand, as described in \textcite{Berger.1996} does not impose the restrictive conditional independence assumption and can be applied when underlying distributions are not know ex ante. 
% c.f. http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/ and \textcite{Pang.2002}
This is achieved in the machine learning context through a training set that produces output values $y$ from a finite set $\mathcal{Y}$ while the inputs $x$ are from $\mathcal{X}$. Their empirical probability distribution is
%
\begin{equation*}
	\tilde{p}(x,y) = \frac{1}{N}\times number\, of\, times\, that\, (x,y)\, occurs\, in\, the\, sample
\end{equation*}
%
where $N$ is the size of the training set. Furthermore, introducing an indicator function, often referred to as \textit{feature},
%
\begin{equation*}
	f_j(x,y) = 
				\begin{cases}
					1 &\text{ if } y = c_i \text{ and $x$ contains $w_k$},\\
					0 & \text{otherwise }
				\end{cases}
\end{equation*}
%
in which $c_i$ denotes a member of the different classes $\mathcal{C}$ possible while $w_k$ is a word. That means the indicator function returns the value one if a document belongs to class $c_i$ and contains the word $w_k$. Based on their features, \textcite{Berger.1996} show that the model $p^*$ should be selected to be as close as possible to uniform according to the ME principle
%
\begin{equation}
	p^* = \underset{p\in \mathcal{C}}{arg\, max} \big(-\sum_{x,y}\tilde{p}(x)p(y|x)log\, p(y|x)\big)
\end{equation}
%
which can be solved through a Lagrangian approach where the multipliers can be estimated through an iterative scaling algorithm.

%
\subsubsection{Knn} % Elements of statistical learning: p. 463

As stated in \textcite[p.~465]{Friedman.2001}, \textit{k}-nearest-neighbours is best applied in settings where every class has a lot of different prototypes and the decision boundary is irregular. Above that, this family of classifiers does not need a model to fit as it is memory-based. It works with a query point $x_0$ for which the \textit{k} training points $x_{(r)},\, r=1,\dots,k$ are found which are the closest to $x_0$, according to some distance metric. In the article classification task, for every row of the test set Corpus, the \textit{k} closest training set vectors, as determined through Euclidean distance, are found and the classification is achieved through majority vote where ties are broken at random. Should there be ties for the \textit{k}th nearest vector, all candidates are included in the vote as stated in the \textit{R} package documentation. 
% http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf


\subsubsection{Support vector machines} % Elements of statistical learning: p. 417

Finally, support vector machines (svm) perform well, according to \textcite[p.~293]{Williams.2011}, on assignments that are non-linear, sparse and high-dimensional. The underlying rationale is to transform the data such that the classes in the training set become linearly separable by a hyperplane. The classification is then conducted by transforming new data in the same way as in the training set and determine on which side of the hyperplane the points of interest lie. A mathematical representation of this concept is not included in this discussion since too many technicalities would have to be introduced; the basics are explained, however, in \textcite[p.~417]{Friedman.2001}. 

%
\subsubsection{Classification and term structure of interest rates}

% Write why when which algo has been preferred (ES have this residuum thing for endog)
All ML algorithms utilised in order to determine the sentiment distinction are performed through the standard functions in the \textit{R} software package; their classifications are listen in table~\vref{tab:Tab_Class}. 
% Reports: https://www.bls.gov/schedule/2008/home.htm
In accordance with \textcite{Ellingsen.2003}, all dates that inherit another important economic release by the Bureau of Labor Statistics such as the employment report, have been excluded from the sample and marked as \textit{R} while those changes that have not been noticed by markets are indicated with a \textit{U}. The information about relevant economic releases was taken from \textcite{BoLS.2017}.
%
% Classification table
\input{chapters/tables_graphs/Tab_Class.tex}
%
It is obvious that the NB model strongly prefers the endogenous classification while knn determines most decisions to be exogenous. ME and SVM appear to be slightly more attentive towards the sentiment in the articles but come to different conclusions as well. Even though the ML learning methods come with a classification certainty\footnote{I chose not to include the probabilities for every article classification by method for purposes of readability. Furthermore, since results diverge this strong, they do not add significant value or credibility to the respective results.}, it is apparent that the endogeneity/exogeneity task is too much of a specialised task as it builds upon very detailed formulations in the texts for which the training sample is simply too small, in particular for such an extensive action set. As the count-based algorithm includes a significance test across articles for every adjustment date, I base the selection for the final classification on the latter  and refer to the most promising of the ML techniques whenever CB returned an ambiguous classification; the list of OMOs with corresponding classification is outlined in table~\vref{tab:FEDfundstgt}. 

Based on their classification, \textcite{Ellingsen.2003} run two regressions to test their model of market interest rates and target adjustments. First, they test their theory whether the relationship between long and short rates differs on policy days and non-policy days. For that purpose, they estimate the regression
%
\begin{equation}
	\label{eq:PvsNP}
	\Delta i_t^n = \alpha_n + (\beta_n^{NP}d_t^{NP}+\beta_n^{P}d_t^{P})\Delta i_t^{3m}+\nu_t^n
\end{equation}
%
where $\Delta i_t$ describes the change in the interest rate on day $t$ and its respective subscript denotes its maturity; $d_t$ are dummy variables for policy and non-policy days and $\nu_t^n$ is the standard idiosyncratic error. Hereby, they try to find evidence for their first hypothesis 
%
\begin{hypothesis}
	\label{H:1}
	For large n, $\beta_n^P$ < $\beta_n^{NP}$.
\end{hypothesis}
%

Secondly, \textcite{Ellingsen.2003} investigate whether the long and short rates behave differently on policy days classified as endogenous or exogenous as well as whether non-policy days have a similar impact as endogenous policy days. Therefore, 
%
\begin{equation}
	\label{eq:EndvsEx}
	\Delta i_t^n = \alpha_n + (\beta_n^{NP}d_t^{NP}+\beta_n^{End}d_t^{End}+\beta_n^{Ex}d_t^{Ex})\Delta i_t^{3m}+\nu_t^n
\end{equation}
%
is estimated where $d_t^{m}$, $m\in \{End;Ex\}$ are dummy variables taking the value of one if the corresponding day has been classified as endogenous or exogenous. From their theoretic model, hypotheses~\ref{H:2} through \ref{H:4} are formulated such that
\begin{hypothesis}
	\label{H:2}
	For large n, $\beta_n^{Ex}$ < 0 < $\beta_n^{End}$,
\end{hypothesis}
%
\begin{hypothesis}
	\label{H:3}
	$\beta_n^{NP} = \beta_n^{End} = 0$ $\forall$ $n$,
\end{hypothesis}
%
\begin{hypothesis}
	\label{H:4}
	$\beta_n^{j}$ is decreasing in n for j = NP, End.
\end{hypothesis}
%

Since this discussion covers a time period with interest rates at the ZLB and central banks on the verge of inability to use their traditional means of interference, other announcements have been delivered such as the various QE programs described in section~\ref{sec:Lit}. As comparable announcements of unconventional monetary policy are not present during the sample period of \textcite{Ellingsen.2003} but impact financial markets as pointed out in \textcite{Hattori.2016}, I run the regressions in equation~\ref{eq:PvsNP} and \ref{eq:EndvsEx} with an additional dummy interaction for a QE coefficient as control. 
% PROBLEM: other stuff (QE) happened in the period, impacts bond rates -> include or neglect?
Even though this should increase the credibility of the model, the sample period is heavily affected by the events of the financial crisis and thus many of the Fed chairman announcements will have triggered market responses as stated in \textcite{Blinder.2010} even though not all of them cannot be controlled for as they are of ambiguous nature or concur with other statements and report releases and hence, their effects cannot be disentangled clearly in such a limited sample size.

%
\subsection{Model performance}

\begin{itemize}
	\item  include confusion matrix/ratio for every classification algorithm
	\item classify ES articles using the CB approach
\end{itemize}
%